{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "\n",
    "# General\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# NLP\n",
    "from nltk.tokenize import LineTokenizer, sent_tokenize\n",
    "from urllib import parse\n",
    "from textblob import TextBlob\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Display\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/Users/seanosier/data/Metis/Wiki/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickling functions\n",
    "def pickle_it(data, filename, python_version=3):\n",
    "    \"\"\"\n",
    "    In:\n",
    "    data = the data you want to pickle (save)\n",
    "    filename = file name where you want to save the data\n",
    "    python_version = the python version where you will be opening the pickle file\n",
    "    \n",
    "    Out:\n",
    "    Saves a pickle file with your data to to the filename you specify\n",
    "    \"\"\"\n",
    "    with open(filename, \"wb\") as picklefile:\n",
    "        pickle.dump(data, picklefile, protocol=python_version)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    \"\"\"\n",
    "    In:\n",
    "    filename = name of the pickle file you want to open (e.g \"my_pickle.pkl\")\n",
    "    \n",
    "    Out:\n",
    "    Opens and returns the content of the picklefile to a variable of your choice\n",
    "    \"\"\"\n",
    "    with open(filename, \"rb\") as picklefile: \n",
    "        return pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load cleaned articles and topics\n",
    "english_articles = load_pickle(data_path + \"english_articles.pkl\")\n",
    "simple_articles = load_pickle(data_path + \"simple_articles.pkl\")\n",
    "topic_pairs = load_pickle(data_path + \"wiki_topic_pairs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113298, 113298, 113298)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm there are an equal number of english articles, simple articles, and topics \n",
    "len(english_articles), len(simple_articles), len(topic_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling \"Kept\" Sentences from English Article to Simple Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sentences(article):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        article = Cleaned wikipedia article text\n",
    "\n",
    "    Out:\n",
    "        sentences = List of sentences in wikipedia article\n",
    "    \"\"\"\n",
    "    lines = LineTokenizer(blanklines='discard').tokenize(\n",
    "        article.replace(\"<br>\", \"\\n\"))\n",
    "    sentences_by_lines = [sent_tokenize(line) for line in lines]\n",
    "    sentences = [sentence for line in sentences_by_lines for sentence in line]\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_sentences_from_english(english_sentences, simple_sentences):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        english_sentences = List of cleaned english article sentences\n",
    "        simple_sentences = List of cleaned simple article sentences\n",
    "    \n",
    "    Out:\n",
    "        selected_sentences = Set of tuples of sentences selected from article most similar to those in the \n",
    "            simple article\n",
    "    \"\"\"\n",
    "    selected_sentences = set()\n",
    "    for sentence in simple_sentences:\n",
    "        closest_sentences_scores = process.extractBests(sentence, english_sentences, scorer=fuzz.token_set_ratio,\\\n",
    "                                                     score_cutoff=65)\n",
    "        if closest_sentences_scores:\n",
    "            selected, scores = zip(*closest_sentences_scores)\n",
    "        else:\n",
    "            selected = set()\n",
    "        selected_sentences = selected_sentences | set(selected)\n",
    "        \n",
    "    return selected_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_labeled_english_sentences(english_article, simple_article):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        english_article = English article to compare\n",
    "        simple_article = Simple article to compare\n",
    "    \n",
    "    Out:\n",
    "        Tuple of:\n",
    "            english_sentences = List of sentences in english article\n",
    "            english_included = List 1 (Yes) or 0 (No) flag of whether or not sentence is \"included\" in simple article\n",
    "    \"\"\"\n",
    "    english_sentences = get_sentences(english_article)\n",
    "    simple_sentences = get_sentences(simple_article)\n",
    "    selected_sentences = select_sentences_from_english(english_sentences, simple_sentences)\n",
    "    english_included = [1 if sentence in selected_sentences else 0 for sentence in english_sentences]\n",
    "    return english_sentences, english_included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Label English article sentences as to whether or not they are \"included\" in the simple article\"\"\"\n",
    "# NOTE: Labeling the english sentences is a VERY slow.\n",
    "# Suggest using the \"Alternate Approach\" below, which store the data in lists seperate from the cell running,\n",
    "# pickles progress every thousand articles, and prints status updates\n",
    "\n",
    "english_sentences_with_labels = [get_labeled_english_sentences(english, simple) for english, simple \\\n",
    "             in zip(english_articles, simple_articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Alternate Approach (Cell 1/2)\n",
    "\"\"\"\n",
    "english_sentences_with_labels1 = []\n",
    "english_sentences_with_labels2 = []\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Alternate Approach (Cell 2/2)\n",
    "\"\"\"\n",
    "list_to_use = english_sentences_with_labels1\n",
    "pickle_file_number = 1\n",
    "\n",
    "for english, simple in zip(english_articles, simple_articles):\n",
    "    \n",
    "    list_to_use.append(get_labeled_english_sentences(english, simple))\n",
    "    \n",
    "    if len(list_to_use) % 1000 == 0:\n",
    "        pickle_it(list_to_use, data_path + \"english_sentences_with_labels\" + str(pickle_file_number) + \".pkl\")\n",
    "        print(len(english_sentences_with_labels1) + len(english_sentences_with_labels2))\n",
    "    \n",
    "    if len(list_to_use) % 50000 == 0:\n",
    "        list_to_use = english_sentences_with_labels2\n",
    "        pickle_file_number = 2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Pickle labeled sentence data\"\"\"\n",
    "#pickle_it(english_sentences_with_labels, data_path + \"english_sentences_with_labels.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Articles to Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_subsections_with_parent(sections_needing_some_merging,\n",
    "                                  section_level=\"==\"):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        sections_needing_some_merging = List of sections including subsections\n",
    "            not grouped with parent section\n",
    "        section_level = Level of parent section \"==\" --> h1, \"===\" --> h2, etc.\n",
    "\n",
    "    Out:\n",
    "        merged_sections = List of sections with subsections grouped with parent\n",
    "    \"\"\"\n",
    "    sections = [section_level + section if i != 0 else section for i, section\n",
    "                in enumerate(sections_needing_some_merging)]\n",
    "    merged_sections = []\n",
    "    for section in sections:\n",
    "        if section.startswith(section_level + \"=\"):  # If child section:\n",
    "            merged_sections[-1] += \"<br><br>\" + section\n",
    "        else:\n",
    "            merged_sections.append(section)\n",
    "\n",
    "    return merged_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_subsections(sections):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        sections = List of sections with subsections grouped with parent\n",
    "\n",
    "    Out:\n",
    "        grouped_subsections = List of sections with nested list of subsections\n",
    "    \"\"\"\n",
    "    grouped_subsections = []\n",
    "    for section in sections:\n",
    "        subsections = section.split(\"<br><br>===\")\n",
    "        subsections = merge_subsections_with_parent(subsections, \"===\")\n",
    "        grouped_subsections.append(list(subsections))\n",
    "\n",
    "    return grouped_subsections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_paragraphs(sections):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        sections = List of sections with nested list of subsections\n",
    "\n",
    "    Out:\n",
    "        grouped_paragraphs = List of sections with nested list of subsections\n",
    "            with nested list of paragraphs\n",
    "    \"\"\"\n",
    "    grouped_paragraphs = []\n",
    "    for section in sections:\n",
    "\n",
    "        subsection_paragraphs = []\n",
    "        for subsection in section:\n",
    "            paragraphs = subsection.split(\"<br><br>\")\n",
    "            subsection_paragraphs.append(list(paragraphs))\n",
    "\n",
    "        grouped_paragraphs.append(subsection_paragraphs)\n",
    "\n",
    "    return grouped_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_paragraph_sentences(sections):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        sections = List of sections with nested list of with nested list of\n",
    "            paragraphs\n",
    "\n",
    "    Out:\n",
    "        grouped_sentences = List of sections with nested list of subsections\n",
    "            with nested list of paragraphs with nested sentences\n",
    "    \"\"\"\n",
    "    grouped_sentences = []\n",
    "    for section in sections:\n",
    "\n",
    "        subsection_sentences = []\n",
    "        for subsection in section:\n",
    "\n",
    "            paragraph_sentences = []\n",
    "            for paragraph in subsection:\n",
    "                sentences = sent_tokenize(paragraph)\n",
    "                paragraph_sentences.append(list(sentences))\n",
    "\n",
    "            subsection_sentences.append(paragraph_sentences)\n",
    "\n",
    "        grouped_sentences.append(subsection_sentences)\n",
    "\n",
    "    return grouped_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences_with_structure(article):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        article = Cleaned wikipedia article text\n",
    "\n",
    "    Out:\n",
    "        sentences_with_structure = List of sections with nested list of\n",
    "            subsections with nested list of paragraphs with nested sentences\n",
    "    \"\"\"\n",
    "    sections = article.split(\"<br><br>==\")\n",
    "    sections = merge_subsections_with_parent(sections, \"==\")\n",
    "    sections_with_subsections = get_subsections(sections)\n",
    "    sections_with_subsections_paragraphs = get_paragraphs(\n",
    "        sections_with_subsections)\n",
    "    sentences_with_structure = get_paragraph_sentences(\n",
    "        sections_with_subsections_paragraphs)\n",
    "\n",
    "    return sentences_with_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sentence_location_data(sentences_with_structure):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        sentences_with_structure = List of sections with nested list of\n",
    "            subsections with nested list of paragraphs with nested sentences\n",
    "\n",
    "    Out:\n",
    "        location_data = List of lists of sentences and their location data.\n",
    "            Data inludes:\n",
    "                - Sentence\n",
    "                - Cumulative section #\n",
    "                - Cumulative subsection #\n",
    "                - Cumulative paragraph #\n",
    "                - Cumulative sentence #\n",
    "                - Cumulative section percentile\n",
    "                - Cumulative section percentile\n",
    "                - Cumulative paragraph percentile\n",
    "                - Cumulative sentence percentile\n",
    "                - Subsection # in section\n",
    "                - Paragraph # in subsection\n",
    "                - Sentence # in paragraph\n",
    "                - Subsection in section percentile\n",
    "                - Paragraph in subsection percentile\n",
    "                - Sentence in paragraph percentile\n",
    "                - Paragraph # in section\n",
    "                - Paragraph in section percentile\n",
    "                - Sentence # in subsection\n",
    "                - Sentence in subsection percentile\n",
    "                - Sentence # in section\n",
    "                - Sentence in section percentile\n",
    "                - Total # of sentences\n",
    "                - Sentence length\n",
    "    \"\"\"\n",
    "    location_data = []\n",
    "\n",
    "    cum_s = 0  # Cumulative section\n",
    "    cum_ss = 0  # Cumulative section\n",
    "    cum_p = 0  # Cumulative paragraph\n",
    "    cum_sent = 0  # Cumulative sentence\n",
    "\n",
    "    cum_s_sent = 0  # Cumulative sentence in section\n",
    "    cum_ss_sent = 0  # Cumulative sentence in subsection\n",
    "    cum_p_sent = 0  # Cumulative sentence in paragraph\n",
    "\n",
    "    cum_s_p = 0  # Cumulative paragraph in section\n",
    "    cum_ss_p = 0  # Cumulative paragraph in subsection\n",
    "\n",
    "    cum_s_ss = 0  # Cumulative subsection in section\n",
    "\n",
    "    # Total sections\n",
    "    total_s = len(sentences_with_structure)\n",
    "    # Total subsections\n",
    "    total_ss = sum([len(section) for section in sentences_with_structure])\n",
    "    # Total paragraphs\n",
    "    total_p = sum([len(subsection) for section in sentences_with_structure\n",
    "                   for subsection in section])\n",
    "    # Total sentences\n",
    "    total_sent = sum([len(paragraph) for section in sentences_with_structure\n",
    "                      for subsection in section for paragraph in subsection])\n",
    "\n",
    "    for s, section in enumerate(sentences_with_structure):\n",
    "        # Total subsections in section\n",
    "        s_total_ss = len(section)\n",
    "        # Total paragraphs in section\n",
    "        s_total_p = sum([len(subsection) for subsection in section])\n",
    "        # Total sentences in section\n",
    "        s_total_sent = sum([len(paragraph) for subsection in section\n",
    "                            for paragraph in subsection])\n",
    "\n",
    "        for ss, subsection in enumerate(section):\n",
    "            # Total paragraphs in subsection\n",
    "            ss_total_p = len(subsection)\n",
    "            # Total sentences in subsection\n",
    "            ss_total_sent = sum([len(paragraph) for paragraph in subsection])\n",
    "\n",
    "            for p, paragraph in enumerate(subsection):\n",
    "                # Total sentences in paragraph\n",
    "                p_total_sent = len(paragraph)\n",
    "\n",
    "                for sent, sentence in enumerate(paragraph):\n",
    "                    sent_len = len(sentence)  # Sentence length\n",
    "                    \"\"\"See docstring above for a list of data definitions\n",
    "                    in order\"\"\"\n",
    "                    location_data.append(\n",
    "                        [sentence,\n",
    "                            cum_s, cum_ss, cum_p, cum_sent,\n",
    "                            cum_s/total_s, cum_ss/total_ss, cum_p/total_p,\n",
    "                            cum_sent/total_sent,\n",
    "\n",
    "                            ss, p, sent,\n",
    "                            ss/s_total_ss, p/ss_total_p, sent/p_total_sent,\n",
    "\n",
    "                            (cum_p - cum_s_p), (cum_p - cum_s_p)/s_total_p,\n",
    "                            (cum_sent - cum_ss_sent),\n",
    "                            (cum_sent - cum_ss_sent)/ss_total_sent,\n",
    "\n",
    "                            (cum_sent - cum_s_sent),\n",
    "                            (cum_sent - cum_s_sent)/s_total_sent,\n",
    "\n",
    "                            total_sent,\n",
    "                            sent_len])\n",
    "\n",
    "                    cum_sent += 1\n",
    "\n",
    "                cum_p_sent = cum_sent\n",
    "                cum_p += 1\n",
    "\n",
    "            cum_ss_p = cum_p\n",
    "            cum_ss_sent = cum_sent\n",
    "            cum_ss += 1\n",
    "\n",
    "        cum_s_ss = cum_ss\n",
    "        cum_s_p = cum_p\n",
    "        cum_s_sent = cum_sent\n",
    "        cum_s += 1\n",
    "\n",
    "    return location_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_topic_mentions(sentence, topic):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        sentence = Cleaned sentence from wikipedia article\n",
    "        topic = Cleaned topic of wikipedia article\n",
    "\n",
    "    Out:\n",
    "        topic_mentions = # of times topic was mentioned in the sentence\n",
    "            (normalized for number of words in the topic)\n",
    "    \"\"\"\n",
    "    topic_words = TextBlob(topic).words.lower()\n",
    "    sent_text = TextBlob(sentence)\n",
    "\n",
    "    topic_mentions = 0\n",
    "    for word in topic_words:\n",
    "        topic_mentions += sent_text.word_counts[word]\n",
    "\n",
    "    try:\n",
    "        topic_mentions = topic_mentions/len(topic_words)\n",
    "    except:\n",
    "        topic_mentions = 0\n",
    "\n",
    "    return topic_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_type_data(sentence):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        sentence = Cleaned sentence from wikipedia article\n",
    "\n",
    "    Out:\n",
    "        subheading = Sentence is a subheading (Yes = 1, No = 0)\n",
    "        heading = Sentence is a heading (Yes = 1, No = 0)\n",
    "        table = Sentence is part of a table (Yes = 1, No = 0)\n",
    "        bullet = Sentence is a bullet (Yes = 1, No = 0)\n",
    "        numbered_bullet = Sentence is a numbered bullet (Yes = 1, No = 0)\n",
    "    \"\"\"\n",
    "    subheading = 0\n",
    "    heading = 0\n",
    "    table = 0\n",
    "    bullet = 0\n",
    "    numbered_bullet = 0\n",
    "\n",
    "    if sentence.startswith(\"===\"):\n",
    "        subheading = 1\n",
    "    elif sentence.startswith(\"==\"):\n",
    "        heading = 1\n",
    "    elif sentence.startswith(\"TABLE:\") or sentence.startswith(\"||\"):\n",
    "        table = 1\n",
    "    elif sentence.startswith(\":*\") or sentence.startswith(\"*\"):\n",
    "        bullet = 1\n",
    "    elif sentence.startswith(\"#\"):\n",
    "        numbered_bullet = 1\n",
    "\n",
    "    return subheading, heading, table, bullet, numbered_bullet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment_data(sentence):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        sentence = Cleaned sentence from wikipedia article\n",
    "\n",
    "    Out:\n",
    "        polarity = Postive / negative sentiment of the sentence\n",
    "        subjectivity = Objectivity / subjectivity sentiment of the sentence\n",
    "    \"\"\"\n",
    "    sentiment = TextBlob(sentence).sentiment\n",
    "    polarity, subjectivity = tuple(sentiment)\n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_article_to_data(article, topic, return_dataframe=True):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        article = Cleaned wikipedia article\n",
    "        topic = Topic of wikipedia article\n",
    "        return_dataframe = Whether or not to return a Pandas DataFrame;\n",
    "            if False, returns a list of lists instead\n",
    "\n",
    "    Out:\n",
    "        Pandas DataFrame of sentence data for the wikipedia article\n",
    "            OR\n",
    "        List of lists of sentence data for the wikipedia article\n",
    "    \"\"\"\n",
    "    sentences = get_sentences(article)\n",
    "    topic = parse.unquote(topic.replace(\"_\", \" \"))\n",
    "\n",
    "    sentences_with_structure = get_sentences_with_structure(article)\n",
    "    sentence_location_data = generate_sentence_location_data(\n",
    "        sentences_with_structure)\n",
    "\n",
    "    article_data_list = []\n",
    "    for sentence, location_data \\\n",
    "            in zip(sentences, sentence_location_data):\n",
    "\n",
    "        sentence_type_data = get_sentence_type_data(sentence)\n",
    "        topic_mentions = get_topic_mentions(sentence, topic)\n",
    "        sentiment_data = get_sentiment_data(sentence)\n",
    "\n",
    "        data_row = [sentence] + list(location_data[1:]) + list(sentence_type_data) +\\\n",
    "            [topic_mentions] + list(sentiment_data)\n",
    "        article_data_list.append(data_row)\n",
    "\n",
    "    if return_dataframe:\n",
    "        column_names = [\"sentence\", \"cum_sect\", \"cum_subsect\", \"cum_para\",\n",
    "                        \"cum_sent\", \"cum_sect_%\", \"cum_subsect_%\",\n",
    "                        \"cum_para_%\", \"cum_sent_%\", \"subsect_in_sect\",\n",
    "                        \"para_in_subsect\", \"sent_in_para\", \"subsect_in_sect_%\",\n",
    "                        \"para_in_subsect_%\", \"sent_in_para_%\",\n",
    "                        \"para_in_section\", \"para_in_section_%\",\n",
    "                        \"sent_in_subsect\", \"sent_in_subsect_%\", \"sent_in_sect\",\n",
    "                        \"sent_in_sect_%\", \"total_sents\", \"sent_len\",\n",
    "                        \"subheading\", \"heading\", \"table\", \"bullet\",\n",
    "                        \"numbered_bullet\", \"topic_mentions\", \"polarity\",\n",
    "                        \"subjectivity\"]\n",
    "        return pd.DataFrame(article_data_list, columns=column_names)\n",
    "    else:\n",
    "        return article_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean up a few instances in the english articles that cause errors\n",
    "english_articles = list(english_articles)\n",
    "english_articles[28802] = english_articles[28802].replace(\"\\x85\", \"<br><br>\")\n",
    "english_articles[60005] = english_articles[60005].replace(\"\\u2029\", \"<br><br>\")\n",
    "english_articles[83640] = english_articles[83640].replace(\"\\x85\", \"<br><br>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_all_articles_to_data(english_articles, topic_pairs):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        english_articles = List of cleaned english articles\n",
    "        topic_pairs = List of topic pairs: (english topic, simple topic)\n",
    "    \n",
    "    Out:\n",
    "        article_data = List of lists of sentences and their associated data for all articles\n",
    "    \"\"\"\n",
    "    article_data = []\n",
    "    i = 0\n",
    "    \n",
    "    for article, topic_pair in zip(english_articles, topic_pairs):\n",
    "        topic = topic_pair[0]\n",
    "        article_data += convert_article_to_data(article, topic, False)\n",
    "        \n",
    "        # Status tracking\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "\n",
    "    print(i)\n",
    "    \n",
    "    return article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert all articles to data\n",
    "article_data = convert_all_articles_to_data(english_articles, topic_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13158106"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Pickle article data\"\"\"\n",
    "#pickle_it(article_data[:6000000], data_path + \"article_data_a.pkl\")\n",
    "#pickle_it(article_data[6000000:], data_path + \"article_data_b.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_pack = load_pickle(data_path + \"prediction_model.pkl\")\n",
    "model = model_pack[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_summary(sentences, paragraph_numbers, included_predictions):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        sentences = List of sentences in article to summarize\n",
    "        paragraph_numbers = List of paragraph numbers of the sentences in\n",
    "            article to summarize\n",
    "        included_predictions = List of 1 (Yes), 0 (No) predictions for whether\n",
    "            or not each sentence is included in the summary\n",
    "\n",
    "    Out:\n",
    "        summary = Summary string for article\n",
    "    \"\"\"\n",
    "    summary_list = [(sentence, paragraph_number)\n",
    "                    for sentence, paragraph_number, included\n",
    "                    in zip(sentences, paragraph_numbers, included_predictions)\n",
    "                    if included == 1]\n",
    "\n",
    "    summary = \"\"\n",
    "    last_paragraph_number = 0\n",
    "    i = 0\n",
    "    for sentence, paragraph_number in summary_list:\n",
    "        if i == 0:\n",
    "            summary += sentence\n",
    "            i += 1\n",
    "        elif paragraph_number > last_paragraph_number:\n",
    "            summary += \"<br><br>\" + sentence\n",
    "        else:\n",
    "            summary += \" \" + sentence\n",
    "        last_paragraph_number = paragraph_number\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_html_tages_to_summary(summary):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        summary = Summary string for article with Wiki markdown\n",
    "\n",
    "    Out:\n",
    "        summary = Summary string for article with Wiki markdown replaced by\n",
    "            HTML tags\n",
    "    \"\"\"\n",
    "    # Bold tags\n",
    "    while re.search(r\"\\'\\'\\'.*?\\'\\'\\'\", summary):\n",
    "        instance = re.search(r\"\\'\\'\\'.*?\\'\\'\\'\", summary).group()\n",
    "        summary = summary.replace(instance, \"<b>\" + instance[3:-3] + \"</b>\")\n",
    "\n",
    "    # Italic tags\n",
    "    while re.search(r\"\\'\\'.*?\\'\\'\", summary):\n",
    "        instance = re.search(r\"\\'\\'.*?\\'\\'\", summary).group()\n",
    "        summary = summary.replace(instance, \"<i>\" + instance[2:-2] + \"</i>\")\n",
    "\n",
    "    # Italic heading tags\n",
    "    while re.search(r\"\\=\\=\\=\\=.*?\\=\\=\\=\\=\", summary):\n",
    "        instance = re.search(r\"\\=\\=\\=\\=.*?\\=\\=\\=\\=\", summary).group()\n",
    "        summary = summary.replace(instance, \"<i>\" + instance[4:-4] + \"</i>\")\n",
    "\n",
    "    # Bold and italic heading tags\n",
    "    while re.search(r\"\\=\\=\\=.*?\\=\\=\\=\", summary):\n",
    "        instance = re.search(r\"\\=\\=\\=.*?\\=\\=\\=\", summary).group()\n",
    "        summary = summary.replace(instance, \"<b><i>\" + instance[3:-3] +\n",
    "                                  \"</i></b>\")\n",
    "\n",
    "    # Bold heading tags\n",
    "    while re.search(r\"\\=\\=.*?\\=\\=\", summary):\n",
    "        instance = re.search(r\"\\=\\=.*?\\=\\=\", summary).group()\n",
    "        summary = summary.replace(instance, \"<b>\" + instance[2:-2] +\n",
    "                                  \"</b>\")\n",
    "\n",
    "    # Clean up any extra spaces\n",
    "    summary = summary.replace(\"<b> \", \"<b>\")\n",
    "    summary = summary.replace(\"<i> \", \"<i>\")\n",
    "    summary = summary.replace(\" </b>\", \"</b>\")\n",
    "    summary = summary.replace(\" </i>\", \"</i>\")\n",
    "\n",
    "    # Unordered list tags\n",
    "    while re.search(r\"\\*.*?(?:\\<br\\>\\<br\\>|$)\", summary):\n",
    "        instance = re.search(r\"\\*.*?(?:\\<br\\>\\<br\\>|$)\", summary).group()\n",
    "        summary = summary.replace(instance, \"<ul><li>\" + instance[1:] +\n",
    "                                  \"</li></ul>\")\n",
    "\n",
    "    # Ordered list tags\n",
    "    while re.search(r\"\\#.*?(?:\\<br\\>\\<br\\>|$)\", summary):\n",
    "        instance = re.search(r\"\\#.*?(?:\\<br\\>\\<br\\>|$)\", summary).group()\n",
    "        summary = summary.replace(instance, \"<ol><li>\" + instance[1:] +\n",
    "                                  \"</li></ol>\")\n",
    "\n",
    "    # Clean up extra unordered list tags\n",
    "    if re.search(r\"\\<ul\\>.*\\<\\/ul\\>\", summary):\n",
    "        instance = re.search(r\"\\<ul\\>.*\\<\\/ul\\>\", summary).group()\n",
    "        new_instance = instance[4:-5].replace(\"<ul>\", \"\")\n",
    "        new_instance = new_instance.replace(\"</ul>\", \"\")\n",
    "        new_instance = new_instance.replace(\"<br>\", \"\")\n",
    "        summary = summary.replace(instance, \"<ul>\" + new_instance + \"</ul>\")\n",
    "\n",
    "    # Clean up extra ordered list tags\n",
    "    if re.search(r\"\\<ol\\>.*\\<\\/ol\\>\", summary):\n",
    "        instance = re.search(r\"\\<ol\\>.*\\<\\/ol\\>\", summary).group()\n",
    "        new_instance = instance[4:-5].replace(\"<ol>\", \"\")\n",
    "        new_instance = new_instance.replace(\"</ol>\", \"\")\n",
    "        new_instance = new_instance.replace(\"<br>\", \"\")\n",
    "        summary = summary.replace(instance, \"<ol>\" + new_instance + \"</ol>\")\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize_article(article, topic):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        article = Cleaned wikipedia article to summarize\n",
    "        topic = Topic of wikipedia article to summarize\n",
    "\n",
    "    Out:\n",
    "        summary = Summary string for article\n",
    "    \"\"\"\n",
    "    df = convert_article_to_data(article, topic)\n",
    "\n",
    "    sentences = df[\"sentence\"]\n",
    "    paragraph_numbers = df[\"cum_para\"]\n",
    "\n",
    "    X = df.drop([\"sentence\"], axis=1)\n",
    "\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    summary = build_summary(sentences, paragraph_numbers, predictions)\n",
    "    summary = add_html_tages_to_summary(summary)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test summary of desired article\n",
    "summarize_article(english_articles[0], topic_pairs[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
