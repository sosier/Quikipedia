{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping / Acquistion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "from wikipedia_page_cleaning import clean_wiki_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/Users/seanosier/data/Metis/Wiki/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickling functions\n",
    "def pickle_it(data, filename, python_version=3):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        data = the data you want to pickle (save)\n",
    "        filename = file name where you want to save the data\n",
    "        python_version = the python version where you will be opening the pickle file\n",
    "    \n",
    "    Out:\n",
    "        Saves a pickle file with your data to to the filename you specify\n",
    "    \"\"\"\n",
    "    with open(filename, \"wb\") as picklefile:\n",
    "        pickle.dump(data, picklefile, protocol=python_version)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        filename = name of the pickle file you want to open (e.g \"my_pickle.pkl\")\n",
    "    \n",
    "    Out:\n",
    "        Opens and returns the content of the picklefile to a variable of your choice\n",
    "    \"\"\"\n",
    "    with open(filename, \"rb\") as picklefile: \n",
    "        return pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_HTML(url):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        url = address of the website whose contents you want to scrape\n",
    "    \n",
    "    Out:\n",
    "        html = the raw HTML of the website for scraping\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    assert (response.status_code >= 200) and (response.status_code < 300)\n",
    "    html = response.text\n",
    "    html = BeautifulSoup(html, \"lxml\")\n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Simple Wiki Article Index (List) Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_simple_wiki_index_page(html):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        html = Raw HTML of wiki index page to scrape\n",
    "    \n",
    "    Out:\n",
    "        Tuple of:\n",
    "            links = List of links to individual simple wiki pages\n",
    "            next_page = URL of next wiki index page\n",
    "    \"\"\"\n",
    "    index_list = html.find(class_=\"mw-allpages-chunk\")\n",
    "    a_tags = index_list.find_all(\"a\")\n",
    "    links = []\n",
    "    for link in a_tags:\n",
    "        links.append(link[\"href\"])\n",
    "    \n",
    "    nav_links = html.find(class_=\"mw-allpages-nav\").find_all(\"a\")\n",
    "    if nav_links[-1].text[:4] == \"Next\":\n",
    "        next_page = nav_links[-1][\"href\"]\n",
    "    else:\n",
    "        next_page = \"\"\n",
    "    \n",
    "    return links, next_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_simple_wiki_links(starting_url):\n",
    "    \"\"\"\n",
    "    In:\n",
    "       starting_url = URL of first wiki index page to scrape \n",
    "    \n",
    "    Out:\n",
    "        all_links = List of links to all individual simple wiki pages\n",
    "    \"\"\"\n",
    "    next_url = starting_url\n",
    "    all_links = []\n",
    "    url_prefix = \"https://simple.wikipedia.org\"\n",
    "    last_save = 0\n",
    "    \n",
    "    while next_url != url_prefix:\n",
    "        html = get_HTML(next_url)\n",
    "        links, next_page = scrape_simple_wiki_index_page(html)\n",
    "        all_links += links\n",
    "        next_url = url_prefix + next_page\n",
    "        \n",
    "        if len(all_links) > 10000 + last_save:\n",
    "            pickle_it((all_links, next_url), data_path + \"simple_wiki_links.pkl\")\n",
    "            last_save = len(all_links)\n",
    "        \n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scrape all simple wiki links\n",
    "all_simple_wiki_links = get_all_simple_wiki_links(\n",
    "    \"https://simple.wikipedia.org/w/index.php?title=Special:AllPages&from=%21%21%21&hideredirects=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115748"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove links to simple wiki pages that were removed since scraping\n",
    "all_simple_wiki_links.remove(\"/wiki/2009-2010_Mary_Fisk_School_screamer_incident\")\n",
    "all_simple_wiki_links.remove(\"/wiki/Eaas\")\n",
    "all_simple_wiki_links.remove(\"/wiki/Hazel_Levesque\")\n",
    "all_simple_wiki_links.remove(\"/wiki/Hoodoo_(geology)\")\n",
    "all_simple_wiki_links.remove(\"/wiki/Truancy\")\n",
    "len(all_simple_wiki_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pickle simple wiki links\n",
    "pickle_it(all_simple_wiki_links, data_path + \"simple_wiki_links.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115748"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract simple wiki topics from simple wiki links\n",
    "simple_topics = [link.split(\"/\")[-1] for link in all_simple_wiki_links]\n",
    "len(simple_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pickle simple wiki topics\n",
    "pickle_it(simple_topics, data_path + \"simple_wiki_topics.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape All Individual Wiki Pages to Get English Wiki Page Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_english_wiki_links(all_simple_wiki_links):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        all_simple_wiki_links = List of links to all individual simple wiki pages\n",
    "    \n",
    "    Out:\n",
    "        all_links = List of links to all individual english wiki pages\n",
    "    \"\"\"\n",
    "    all_links = []\n",
    "    url_prefix = \"https://simple.wikipedia.org\"\n",
    "    last_save = 0\n",
    "    \n",
    "    for link in all_simple_wiki_links:\n",
    "        html = get_HTML(url_prefix + link)\n",
    "        try:\n",
    "            english_wiki_link_li = html.find(class_=\"interwiki-en\")\n",
    "            all_links.append(english_wiki_link_li.find(\"a\")[\"href\"])\n",
    "        except:\n",
    "            all_links.append(\"\")\n",
    "        \n",
    "        if len(all_links) >= 100 + last_save:\n",
    "            pickle_it(all_links, data_path + \"english_wiki_links.pkl\")\n",
    "            last_save = len(all_links)\n",
    "            print(last_save)\n",
    "        \n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115748"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape English Wiki Links from Simple Wiki Pages\n",
    "english_wiki_links = get_english_wiki_links(all_simple_wiki_links)\n",
    "len(english_wiki_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pickle english wiki links\n",
    "pickle_it(english_wiki_links, data_path + \"english_wiki_links.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115748"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract english wiki topics from english wiki links\n",
    "english_topics = [link.split(\"/\")[-1] for link in english_wiki_links]\n",
    "len(english_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pickle english wiki topics \n",
    "pickle_it(english_topics, data_path + \"english_wiki_topics.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113420"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group english topics and simple topics together, and remove any without english pages\n",
    "topic_pairs = [(english, simple) for english, simple in zip(english_topics, simple_topics) if english != \"\"]\n",
    "len(topic_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pickle english, simple wiki topic pairs \n",
    "pickle_it(topic_pairs, data_path + \"wiki_topic_pairs.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Raw English and Simple Wiki Articles Using API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pull_raw_wiki_articles(topic_pairs):\n",
    "    \"\"\"\n",
    "    In:\n",
    "        topic_pairs = List of grouped english topics and simple topics together\n",
    "    \n",
    "    Out:\n",
    "        raw_english_articles = Raw text for all english articles\n",
    "        raw_simple_articles = Raw text for all simple articles\n",
    "    \"\"\"\n",
    "    raw_english_articles = []\n",
    "    raw_simple_articles = []\n",
    "    pickle_number = 1\n",
    "    \n",
    "    for pair in topic_pairs:\n",
    "        english_topic = pair[0]\n",
    "        simple_topic = pair[1]\n",
    "        \n",
    "        # Get english article text\n",
    "        english_url = \"https://en.wikipedia.org/w/index.php?action=raw&title=\" + english_topic\n",
    "        try:\n",
    "            raw_english_text = urlopen(english_url)\n",
    "            raw_english_text = raw_english_text.read().decode('UTF-8')\n",
    "        except:\n",
    "            raw_english_text = \"\"\n",
    "        \n",
    "        # Get simple article text\n",
    "        simple_url = \"https://simple.wikipedia.org/w/index.php?action=raw&title=\" + simple_topic\n",
    "        try:\n",
    "            raw_simple_text = urlopen(simple_url)\n",
    "            raw_simple_text = raw_simple_text.read().decode('UTF-8')\n",
    "        except:\n",
    "            raw_simple_text = \"\"\n",
    "        \n",
    "        # Deal with any english article redirects\n",
    "        if raw_english_text[:9] == \"#REDIRECT\":\n",
    "            english_topic = re.search(r\"\\[\\[.*\\]\\]\", raw_english_text).group()[2:-2]\n",
    "            english_topic = english_topic.replace(\" \", \"_\")\n",
    "            english_url = \"https://en.wikipedia.org/w/index.php?action=raw&title=\" + english_topic\n",
    "            try:\n",
    "                raw_english_text = urlopen(english_url)\n",
    "                raw_english_text = raw_english_text.read().decode('UTF-8')\n",
    "            except:\n",
    "                raw_english_text = \"\"\n",
    "        \n",
    "        # Deal with any simple article redirects\n",
    "        if raw_simple_text[:9] == \"#REDIRECT\":\n",
    "            simple_topic = re.search(r\"\\[\\[.*\\]\\]\", raw_simple_text).group()[2:-2]\n",
    "            simple_topic = simple_topic.replace(\" \", \"_\")\n",
    "            simple_url = \"https://simple.wikipedia.org/w/index.php?action=raw&title=\" + simple_topic\n",
    "            try:\n",
    "                raw_simple_text = urlopen(simple_url)\n",
    "                raw_simple_text = raw_simple_text.read().decode('UTF-8')\n",
    "            except:\n",
    "                raw_simple_text = \"\"\n",
    "            \n",
    "        raw_english_articles.append(raw_english_text)\n",
    "        raw_simple_articles.append(raw_simple_text)\n",
    "        \n",
    "        # Give status updated and save progress periodically\n",
    "        if len(raw_english_articles) % 30000 == 0:\n",
    "            pickle_number += 1\n",
    "        \n",
    "        if len(raw_english_articles) % 1000 == 0:\n",
    "            pickle_it((raw_english_articles, raw_simple_articles), data_path + \"raw_wiki_articles\" \\\n",
    "                      + str(pickle_number) + \".pkl\")\n",
    "            print(len(raw_english_articles))\n",
    "        \n",
    "    return raw_english_articles, raw_simple_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113420 113420\n"
     ]
    }
   ],
   "source": [
    "# Pull raw text for all english and simple article pairs\n",
    "raw_english_articles, raw_simple_articles = pull_raw_wiki_articles(topic_pairs)\n",
    "print(len(raw_english_articles), len(raw_simple_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113325"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove any english article, simple article, topic trios if either the english or simple article is blank\n",
    "raw_article_pairs = [(english, simple, topic) for english, simple, topic \\\n",
    "                     in zip(raw_english_articles, raw_simple_articles, topic_pairs)\\\n",
    "                     if english != \"\" and simple != \"\"]\n",
    "len(raw_article_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unzip the remaining english articles, simple articles, and topics\n",
    "raw_english_articles, raw_simple_articles, topic_pairs = zip(*raw_article_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split english and simple article lists into two and pickle (full list won't fit in a single pickle)\n",
    "pickle_it(raw_english_articles[:50000], data_path + \"raw_english_articles1.pkl\")\n",
    "pickle_it(raw_english_articles[50000:], data_path + \"raw_english_articles2.pkl\")\n",
    "\n",
    "pickle_it(raw_simple_articles[:50000], data_path + \"raw_simple_articles1.pkl\")\n",
    "pickle_it(raw_simple_articles[50000:], data_path + \"raw_simple_articles2.pkl\")\n",
    "\n",
    "pickle_it(topic_pairs, data_path + \"wiki_topic_pairs.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Raw Wiki Article Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean simple articles\n",
    "simple_articles = [clean_wiki_page(article) for article in raw_simple_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113325"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simple_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle cleaned simple articles\n",
    "pickle_it(simple_articles, data_path + \"simple_articles.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove last character in this english article because it causes issues during article cleaning\n",
    "raw_english_articles = list(raw_english_articles)\n",
    "raw_english_articles[83399] = raw_english_articles[83399][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clean english articles\n",
    "english_articles = [clean_wiki_page(article) for article in raw_english_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113325"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle cleaned simple articles\n",
    "pickle_it(english_articles, data_path + \"english_articles.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113298"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove any english article, simple article, topic trios if either the cleaned english or simple article is blank\n",
    "article_pairs_and_topics = [(english, simple, topic) for english, simple, topic \n",
    "                            in zip(english_articles, simple_articles, topic_pairs)\\\n",
    "                            if english != \"\" and simple != \"\"]\n",
    "len(article_pairs_and_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip the remaining english articles, simple articles, and topics\n",
    "english_articles, simple_articles, topic_pairs = zip(*article_pairs_and_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113298, 113298, 113298)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_articles), len(simple_articles), len(topic_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle final, cleaned english articles, simple articles, and topic pairs\n",
    "pickle_it(english_articles, data_path + \"english_articles.pkl\")\n",
    "pickle_it(simple_articles, data_path + \"simple_articles.pkl\")\n",
    "pickle_it(topic_pairs, data_path + \"wiki_topic_pairs.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
